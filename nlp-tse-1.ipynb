{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_venn as venn\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "\n",
    "\n",
    "# sklearn \n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "stop=set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "#Avoid warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#plotly libraries\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.subplots import make_subplots\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    if (len(a) + len(b) - len(c)) !=0:\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we dont have length larger than 96\n",
    "MAX_LEN = 96\n",
    "\n",
    "# Pretrained model of roberta\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "# Sentiment ID value is encoded from tokenizer\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "ct=train.shape[0] #27481\n",
    "\n",
    "# Initialising training inputs\n",
    "input_ids=np.ones((ct,MAX_LEN),dtype=\"int32\")          # Array with value 1 of shape(27481,96)\n",
    "attention_mask=np.zeros((ct,MAX_LEN),dtype=\"int32\")    # Array with value 0 of shape(27481,96)\n",
    "token_type_ids=np.zeros((ct,MAX_LEN),dtype=\"int32\")    # Array with value 0 of shape(27481,96)\n",
    "start_tokens=np.zeros((ct,MAX_LEN),dtype=\"int32\")      # Array with value 0 of shape(27481,96)\n",
    "end_tokens=np.zeros((ct,MAX_LEN),dtype=\"int32\")        # Array with value 0 of shape(27481,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(train.shape[0]):\n",
    "#1 FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    \n",
    "    # idx - position where the selected text are placed. \n",
    "    idx = text1.find(text2)   # we get [12] position\n",
    "    \n",
    "    # all character position as 0 and then places 1 for selected text position  \n",
    "    chars = np.zeros((len(text1))) \n",
    "    chars[idx:idx+len(text2)]=1    # [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] \n",
    "    \n",
    "    #tokenize id of text \n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1    \n",
    "    enc = tokenizer.encode(text1)  #  [127, 3504, 16, 11902, 162]\n",
    "        \n",
    "#2. ID_OFFSETS - start and end index of text\n",
    "    offsets = []\n",
    "    idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))     #  [(0, 3), (3, 8), (8, 11), (11, 20), (20, 23)]\n",
    "        idx += len(w) \n",
    "    \n",
    "#3  START-END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b]) # number of characters in selected text - [0.0,0.0,0.0,9.0,3.0] - bullying me\n",
    "        if sm>0: \n",
    "            toks.append(i)  # token position - selected text - [3, 4]\n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']] # Encoded values by tokenizer\n",
    "    \n",
    "    #Formating input for roberta model\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]   #[ 0   127  3504    16 11902   162     2     2  2430     2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1                                  # [1 1 1 1 1 1 1 1 1 1]\n",
    "    \n",
    "    if len(toks)>0:\n",
    "        # this will produce (27481, 96) & (27481, 96) arrays where tokens are placed\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct_test = test.shape[0]\n",
    "\n",
    "# Initialize inputs\n",
    "input_ids_t = np.ones((ct_test,MAX_LEN),dtype='int32')        # array with value 1 for shape (3534, 96)\n",
    "attention_mask_t = np.zeros((ct_test,MAX_LEN),dtype='int32')  # array with value 0 for shape (3534, 96)\n",
    "token_type_ids_t = np.zeros((ct_test,MAX_LEN),dtype='int32')  # array with value 0 for shape (3534, 96)\n",
    "\n",
    "# Set Inputs attention \n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "#1. INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "     \n",
    "    # Encoded value of tokenizer\n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    \n",
    "    #setting up of input ids - same as we did for train\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5 * 0.2**epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    \n",
    "    # Initialize keras layers\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    # Fetching pretrained models \n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    # Setting up layers\n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    # Initializing input,output for model.THis will be trained in next code\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    \n",
    "    #Adam optimizer for stochastic gradient descent. if you are unware of it - https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 0.0352 - activation_loss: 0.0178 - activation_1_loss: 0.0174\n",
      "Epoch 00001: val_loss improved from inf to 0.02812, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 290s 13ms/sample - loss: 0.0352 - activation_loss: 0.0178 - activation_1_loss: 0.0174 - val_loss: 0.0281 - val_activation_loss: 0.0143 - val_activation_1_loss: 0.0138\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 0.0272 - activation_loss: 0.0138 - activation_1_loss: 0.0134\n",
      "Epoch 00002: val_loss improved from 0.02812 to 0.02729, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 273s 12ms/sample - loss: 0.0272 - activation_loss: 0.0138 - activation_1_loss: 0.0134 - val_loss: 0.0273 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0134\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 0.0250 - activation_loss: 0.0128 - activation_1_loss: 0.0122\n",
      "Epoch 00003: val_loss did not improve from 0.02729\n",
      "21984/21984 [==============================] - 271s 12ms/sample - loss: 0.0250 - activation_loss: 0.0128 - activation_1_loss: 0.0122 - val_loss: 0.0277 - val_activation_loss: 0.0143 - val_activation_1_loss: 0.0134\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.705356014325949\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0339 - activation_loss: 0.0173 - activation_1_loss: 0.0166\n",
      "Epoch 00001: val_loss improved from inf to 0.02784, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 0.0339 - activation_loss: 0.0173 - activation_1_loss: 0.0166 - val_loss: 0.0278 - val_activation_loss: 0.0141 - val_activation_1_loss: 0.0138\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0273 - activation_loss: 0.0139 - activation_1_loss: 0.0134\n",
      "Epoch 00002: val_loss improved from 0.02784 to 0.02719, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 0.0273 - activation_loss: 0.0139 - activation_1_loss: 0.0134 - val_loss: 0.0272 - val_activation_loss: 0.0137 - val_activation_1_loss: 0.0135\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0254 - activation_loss: 0.0130 - activation_1_loss: 0.0125\n",
      "Epoch 00003: val_loss improved from 0.02719 to 0.02654, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 0.0254 - activation_loss: 0.0130 - activation_1_loss: 0.0125 - val_loss: 0.0265 - val_activation_loss: 0.0134 - val_activation_1_loss: 0.0131\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7108600956257223\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0343 - activation_loss: 0.0175 - activation_1_loss: 0.0169\n",
      "Epoch 00001: val_loss improved from inf to 0.02798, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 309s 14ms/sample - loss: 0.0343 - activation_loss: 0.0174 - activation_1_loss: 0.0169 - val_loss: 0.0280 - val_activation_loss: 0.0143 - val_activation_1_loss: 0.0137\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0272 - activation_loss: 0.0139 - activation_1_loss: 0.0133\n",
      "Epoch 00002: val_loss improved from 0.02798 to 0.02695, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 0.0272 - activation_loss: 0.0140 - activation_1_loss: 0.0135 - val_loss: 0.0270 - val_activation_loss: 0.0136 - val_activation_1_loss: 0.0134\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0254 - activation_loss: 0.0130 - activation_1_loss: 0.0125\n",
      "Epoch 00003: val_loss did not improve from 0.02695\n",
      "21985/21985 [==============================] - 287s 13ms/sample - loss: 0.0254 - activation_loss: 0.0130 - activation_1_loss: 0.0125 - val_loss: 0.0270 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0132\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7037445771733006\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0344 - activation_loss: 0.0174 - activation_1_loss: 0.0170\n",
      "Epoch 00001: val_loss improved from inf to 0.02819, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 0.0344 - activation_loss: 0.0174 - activation_1_loss: 0.0169 - val_loss: 0.0282 - val_activation_loss: 0.0143 - val_activation_1_loss: 0.0139\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0274 - activation_loss: 0.0140 - activation_1_loss: 0.0134\n",
      "Epoch 00002: val_loss improved from 0.02819 to 0.02669, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 0.0274 - activation_loss: 0.0140 - activation_1_loss: 0.0134 - val_loss: 0.0267 - val_activation_loss: 0.0135 - val_activation_1_loss: 0.0131\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0253 - activation_loss: 0.0130 - activation_1_loss: 0.0124\n",
      "Epoch 00003: val_loss did not improve from 0.02669\n",
      "21985/21985 [==============================] - 287s 13ms/sample - loss: 0.0253 - activation_loss: 0.0130 - activation_1_loss: 0.0124 - val_loss: 0.0271 - val_activation_loss: 0.0136 - val_activation_1_loss: 0.0136\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.703134604149284\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0346 - activation_loss: 0.0175 - activation_1_loss: 0.0171\n",
      "Epoch 00001: val_loss improved from inf to 0.02736, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 0.0346 - activation_loss: 0.0175 - activation_1_loss: 0.0170 - val_loss: 0.0274 - val_activation_loss: 0.0141 - val_activation_1_loss: 0.0133\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0276 - activation_loss: 0.0139 - activation_1_loss: 0.0136\n",
      "Epoch 00002: val_loss improved from 0.02736 to 0.02725, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 289s 13ms/sample - loss: 0.0276 - activation_loss: 0.0140 - activation_1_loss: 0.0136 - val_loss: 0.0272 - val_activation_loss: 0.0139 - val_activation_1_loss: 0.0133\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 0.0260 - activation_loss: 0.0132 - activation_1_loss: 0.0129\n",
      "Epoch 00003: val_loss improved from 0.02725 to 0.02669, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 0.0261 - activation_loss: 0.0132 - activation_1_loss: 0.0129 - val_loss: 0.0267 - val_activation_loss: 0.0138 - val_activation_1_loss: 0.0129\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7077103676236014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> OVERALL 5Fold CV Jaccard = 0.7061611317795714\n"
     ]
    }
   ],
   "source": [
    "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time=dt.now()\n",
    "\n",
    "# n_splits=5 # Number of splits\n",
    "\n",
    "# # INitialize start and end token\n",
    "# preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "# preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "# DISPLAY=1\n",
    "# for i in range(5):\n",
    "#     print('#'*40)\n",
    "#     print('### MODEL %i'%(i+1))\n",
    "#     print('#'*40)\n",
    "    \n",
    "#     K.clear_session()\n",
    "#     #model = build_model()\n",
    "#     # Pretrained model\n",
    "#     #model.load_weights('../input/model4/v4-roberta-%i.h5'%i)\n",
    "\n",
    "#     print('Predicting Test...')\n",
    "#     #preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "#     preds = m.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "#     preds_start += preds[0]/n_splits\n",
    "#     preds_end += preds[1]/n_splits\n",
    "    \n",
    "# end_time=dt.now()\n",
    "# print(\"   \")\n",
    "# print(\"   \")\n",
    "# print(\"Time Taken to run above code :\",(end_time-start_time).total_seconds()/60,\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    # Argmax - Returns the indices of the maximum values along axis\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>last session of the day http://twitpic.com/67ezh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>such a shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>i like it!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                      selected_text\n",
       "0  f87dea47db   last session of the day http://twitpic.com/67ezh\n",
       "1  96d74cb729                                           exciting\n",
       "2  eee518ae67                                      such a shame!\n",
       "3  01082688c6                                        happy bday!\n",
       "4  33987a8ee5                                        i like it!!"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "submission=test[['textID','selected_text']]\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
